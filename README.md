# Predicting Text Difficulty

## 1. Project Overview

### 1.1. Project Team

**UMICH MADS Milestone II Team**:

- **Andre Buser** (busera; AB)
- **Victor Adafinoaiei** (vadafino; VA)

### 1.2. Introduction and Objective

This project aims to classify sentences from articles in the "Simple English Wikipedia" to determine whether they require simplification to enhance their accessibility and comprehensibility for audiences with low reading proficiency, including students, children, and non-native English speakers. The project employs supervised and unsupervised learning techniques to extract and create features for sentence classification.

The motivation is to leverage the insights and models generated by the project to improve the readability and comprehensibility of our internal investigation and audit reports. By identifying the factors that make a sentence challenging to read, we can develop targeted strategies for enhancing the clarity and accessibility of our written materials, thereby improving the effectiveness of our communication.

The complete report can be accessed here: [Predicting Text Difficulty Report](https://github.com/busera/mads__predicting_text_difficulty/blob/main/reports/Predicting_Text_Difficulty-Final_Report.pdf)

## 2. Datasets

### 2.1. Primary Dataset

The primary datasets were downloaded from the Kaggle competition page:

| Dataset | Description | Size | Format |
|---------|-------------|------|--------|
| WikiLarge_Train.csv | Labeled examples for training a machine learning model | 416,768 records | CSV |
| WikiLarge_Test.csv | Unlabeled examples for model evaluation | 119,092 records | CSV |

### 2.2. Secondary Datasets

| Dataset | Description | Size | Format |
|---------|-------------|------|--------|
| Concreteness_ratings_Brysbaert_et_al_BRM.txt | Concreteness ratings for 40,000 English lemma words gathered via Amazon Mechanical Turk | 39,954 records | TXT |
| AoA_51715_words.csv | "Age of Acquisition" (AoA) estimates for about 51,000 English words | 51,715 records | CSV |

## 3. Methodology

1. Data Cleaning and Understanding
2. Feature Engineering
3. Model Development
   - Supervised Machine Learning
   - Unsupervised Machine Learning
4. Model Evaluation
   - Learning Curve Analysis
   - Sensitivity Analysis
5. Hyperparameter Optimization
6. Failure Analysis

## 4. Key Findings

1. The Random Forest algorithm was the best-performing model for sentence classification with an optimized accuracy score of 0.7544 using 125 of the 151 constructed features.
2. The primary source dataset contained samples in over 89 languages and many non-sentence samples, contrary to initial expectations.
3. Dataset quality and labeling issues presented significant challenges that need to be addressed in future work.

## 5. Feature Engineering

151 unique features were developed through a combination of external resources, intuition, and established best practices for text processing and classification. These features were grouped into six categories:

| Feature Group | Number of Features | Description |
|---------------|---------------------|-------------|
| Text statistics | 33 | Features based on text before and after cleaning activities |
| Readability scores | 31 | Features created using various readability methods and interactions |
| NLTK features | 36 | Features created based on methods from the NLTK package |
| AOA features | 40 | Features created based on the AOA dataset |
| CRB features | 8 | Features created based on the CRB dataset |
| Vector features | 3 | Used fastText and Gensim packages to create vector-based features |

## 6. Model Development

The final model development workflow included:

1. Data preprocessing
2. Feature selection
3. Model training and evaluation
4. Hyperparameter optimization
5. Final model selection

## 7. Evaluation and Analysis

- Supervised Evaluation: Used classification metrics including accuracy, recall, precision, F1 score, ROC-AUC, and confusion matrix.
- Unsupervised Evaluation: Utilized Principal Component Analysis (PCA) and KMeans clustering for feature importance and failure analysis.
- Learning Curve Analysis: Examined the relationship between the number of training samples and the model's accuracy.
- Sensitivity Analysis: Explored the effects of varying key parameters on model performance.

## 8. Ethical Considerations

The project team maintained privacy and data security by using third-party platforms like Google Collab and Trello for source code development and project management while adhering to appropriate access and privacy controls.

## 9. Installation and Setup

### Environment Setup

We recommend using `conda` as the foundation because it simplifies the management of required Python versions.

To create the project's conda environment use:

```bash
conda env create -f environment.yml
```

Once the environment is created, activate it:

```bash
conda activate text_difficulty
```

## 10. File Structure

The project has the following structure:

```
.
├── .git
├── .gitattributes
├── .gitignore
├── data
│   └── environment.yml
├── LICENSE
├── models
├── notebooks
├── README.md
├── reports
│   ├── feature_list.csv
│   ├── figures
│   ├── html
│   ├── Predicting_Text_Difficulty-Final_Report.pdf
│   └── snapshots
```

Key directories and files:
- `data/`: Contains the dataset files and environment configuration.
- `models/`: Stores trained machine learning models.
- `notebooks/`: Jupyter notebooks for data analysis and model development.
- `reports/`: Contains project reports, figures, and additional documentation.
  - `feature_list.csv`: List of features used in the project.
  - `figures/`: Visualizations and plots generated during analysis.
  - `html/`: HTML reports or documentation.
  - `Predicting_Text_Difficulty-Final_Report.pdf`: The final report.
  - `snapshots/`: Snapshots of interim results or model states.
- `environment.yml`: Conda environment configuration file.
- `LICENSE`: The project's license file.
- `README.md`: This file, providing an overview of the project.

## 11. Future Work

Future iterations of the project should focus on:

1. Addressing dataset quality issues
2. Improving the labeling process
3. Exploring additional feature engineering techniques
4. Investigating other machine learning algorithms
5. Developing strategies for simplifying complex sentences based on the model's insights


## 12. Conclusion

The project aimed to classify sentences from "Simple English Wikipedia" articles to determine if they require simplification for improved accessibility. Our analysis yielded several important insights:

1. **Model Performance**: The Random Forest algorithm emerged as the best-performing model, achieving an optimized accuracy score of 0.7544 using 125 of the 151 constructed features. This performance suggests that the model has potential for practical applications in text simplification tasks.

2. **Dataset Challenges**: Our failure analysis revealed significant challenges with the quality of the primary source dataset. Contrary to our initial expectations of only English language samples, we found that the dataset contained samples in over 89 languages. Additionally, many non-sentence samples were present, including image captions, formulas, file names, and single words.

3. **Data Cleaning Limitations**: Despite our efforts to address these issues, such as removing non-sentence samples and filtering out non-English samples, we were unable to improve the model's accuracy. This suggests that the dataset may have underlying labeling issues, which was confirmed by our failure analysis.

4. **Feature Engineering**: The project developed 151 unique features through a combination of external resources, intuition, and established best practices for text processing and classification. However, none of the engineered features could clearly separate the sentences into simple or complex categories, further indicating potential issues with the dataset or labeling process.

5. **Unsupervised Learning Insights**: Our use of unsupervised learning techniques, particularly Principal Component Analysis (PCA) and KMeans clustering, provided valuable insights into feature importance and aided in our failure analysis.

6. **Learning Curve and Sensitivity Analysis**: These analyses demonstrated that our model is robust and can operate effectively within a range of parameter values. The learning curve suggested that the model could potentially benefit from additional training samples.

While the Random Forest algorithm produced the best results, the quality of the dataset and labeling issues present significant challenges that need to be addressed in future work. These findings highlight the importance of data quality and proper labeling in machine learning projects, especially in natural language processing tasks.

The insights gained from this project can be valuable for improving the readability and accessibility of written materials across various domains. However, future work should focus on:

1. Improving dataset quality and labeling processes
2. Exploring more sophisticated feature engineering techniques
3. Investigating alternative machine learning algorithms
4. Developing strategies for simplifying complex sentences based on the model's insights
5. Expanding the analysis to include complete paragraphs instead of isolated sentences

By addressing these areas, future iterations of this project could significantly advance the field of automated text simplification and contribute to more effective communication in various contexts, including education, public information, and technical documentation.

The complete report can be accessed here: [Predicting Text Difficulty Report](https://github.com/busera/mads__predicting_text_difficulty/blob/main/reports/Predicting_Text_Difficulty-Final_Report.pdf)